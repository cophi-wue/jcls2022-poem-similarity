{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas\n",
    "import requests\n",
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import Transformer\n",
    "from sentence_transformers.models import WordEmbeddings\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_file(url, output):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(output, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "poems_dir = './corpus'\n",
    "poems_metadata = pandas.read_csv('./lyrik_metadata.tsv', sep='\\t', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenized_chunks_gen(model, tqdmargs=dict()):\n",
    "    for id in tqdm(poems_metadata.index, **tqdmargs):\n",
    "        content = open(poems_dir + '/' + id + '.txt').read().strip()\n",
    "        content = poems_metadata.loc[id, 'Titel'] + ' ' + content\n",
    "        tokenized = model.tokenizer.tokenize(content)\n",
    "        if len(tokenized) > model.get_max_seq_length():\n",
    "            continue\n",
    "        yield id, content\n",
    "\n",
    "def make_embedding_from_sentence_transformer(model, name, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    chunksize = 16\n",
    "    filenames = []\n",
    "    vectors = []\n",
    "    for chunk in more_itertools.chunked(tokenized_chunks_gen(model, tqdmargs={'desc': name}), n=chunksize):\n",
    "        for x in chunk:\n",
    "            assert len(x) < model.get_max_seq_length()\n",
    "        filenames.extend([x[0] for x in chunk])\n",
    "        vectors.extend(model.encode([x[1] for x in chunk], convert_to_tensor=True))\n",
    "\n",
    "    emb = KeyedVectors(vectors[0].shape[0])\n",
    "    emb.add_vectors(filenames, np.array(torch.stack(vectors).cpu()))\n",
    "    emb.save(f'./embeddings/{name}.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_embedding_from_sentence_transformer(SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1'),\n",
    "                                         'paraphrase-XLM-R', 'cuda')\n",
    "make_embedding_from_sentence_transformer(SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2'),\n",
    "                                         'paraphrase-mpnet', 'cuda')\n",
    "make_embedding_from_sentence_transformer(SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'),\n",
    "                                         'paraphrase-MiniLM', 'cuda')\n",
    "cross_en_de_roberta = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "cross_en_de_roberta._first_module().max_seq_length = 128\n",
    "make_embedding_from_sentence_transformer(cross_en_de_roberta, 'cross-en-de-roberta', 'cuda')\n",
    "del cross_en_de_roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gbert_hidden = Transformer('deepset/gbert-base', model_args={'output_hidden_states': True})\n",
    "for pname, pfn in zip(['mean', 'median', 'meannorm'], [util.pooling_mean, util.pooling_median, util.pooling_meannorm]):\n",
    "    pooling = util.BERTHiddenPooling(gbert_hidden.get_word_embedding_dimension(), layers=[-1], pooling_method=pfn)\n",
    "    model = SentenceTransformer(modules=[gbert_hidden, pooling], device='cuda').cuda()\n",
    "    make_embedding_from_sentence_transformer(model, f'gbert-base-{pname}', 'cuda')\n",
    "\n",
    "for pname, pfn in zip(['mean', 'median', 'meannorm'], [util.pooling_mean, util.pooling_median, util.pooling_meannorm]):\n",
    "    pooling = util.BERTHiddenPooling(gbert_hidden.get_word_embedding_dimension(), layers=list(range(13)), pooling_method=pfn)\n",
    "    model = SentenceTransformer(modules=[gbert_hidden, pooling], device='cuda').cuda()\n",
    "    make_embedding_from_sentence_transformer(model, f'gbert-base-alllayers-{pname}', 'cuda')\n",
    "\n",
    "del gbert_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./models/vectors-deepset-glove.txt'):\n",
    "    download_file('https://int-emb-glove-de-wiki.s3.eu-central-1.amazonaws.com/vectors.txt', './models/vectors-deepset-glove.txt')\n",
    "\n",
    "glove = WordEmbeddings.from_text_file(embeddings_file_path='./models/vectors-deepset-glove.txt', tokenizer=util.WordTokenizer(stop_words=stopwords.words('german')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pname, pfn in zip(['mean', 'median', 'meannorm'], [util.pooling_mean, util.pooling_median, util.pooling_meannorm]):\n",
    "    pooling = util.CustomPooling(glove.get_word_embedding_dimension(), pooling_method=pfn)\n",
    "    model = SentenceTransformer(modules=[glove, pooling], device='cpu')\n",
    "    make_embedding_from_sentence_transformer(model, f'glove-{pname}', 'cpu')\n",
    "\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NOTICE: Poem embeddings for the Fasttext language model are already provided in the repository.\n",
    "\n",
    "# def load_fasttext(filepath):\n",
    "#     fIn = open(filepath)\n",
    "#     fIn.readline()\n",
    "#     iterator = tqdm(fIn, desc=\"Load Word Embeddings\", unit=\"Embeddings\")\n",
    "#     embeddings_dimension = None\n",
    "#     vocab = []\n",
    "#     embeddings = []\n",
    "#     for line in iterator:\n",
    "#         split = line.rstrip().split(' ')\n",
    "#         word = split[0]\n",
    "#\n",
    "#         if embeddings_dimension == None:\n",
    "#             embeddings_dimension = len(split) - 1\n",
    "#             vocab.append(\"PADDING_TOKEN\")\n",
    "#             embeddings.append(np.zeros(embeddings_dimension))\n",
    "#\n",
    "#         if (len(split) - 1) != embeddings_dimension:\n",
    "#             print(\"ERROR: A line in the embeddings file had more or less  dimensions than expected. Skip token.\")\n",
    "#             continue\n",
    "#\n",
    "#         vector = np.array([float(num) for num in split[1:]])\n",
    "#         embeddings.append(vector)\n",
    "#         vocab.append(word)\n",
    "#\n",
    "#     tokenizer = util.WordTokenizer(stop_words=stopwords.words('german'))\n",
    "#     embeddings = np.asarray(embeddings)\n",
    "#     tokenizer.set_vocab(vocab)\n",
    "#     return WordEmbeddings(tokenizer=tokenizer, embedding_weights=embeddings)\n",
    "#\n",
    "#\n",
    "# if not os.path.exists('./models/vectors-cohure-fasttext.txt'):\n",
    "#     download_file('TODO', './models/vectors-cohure-fasttext.txt')\n",
    "#\n",
    "# fasttext = load_fasttext('./models/vectors-cohure-fasttext.txt')\n",
    "#\n",
    "# for pname, pfn in zip(['mean', 'median', 'meannorm'], [util.pooling_mean, util.pooling_median, util.pooling_meannorm]):\n",
    "#     pooling = util.CustomPooling(fasttext.get_word_embedding_dimension(), pooling_method=pfn)\n",
    "#     model = SentenceTransformer(modules=[fasttext, pooling], device='cpu')\n",
    "#     make_embedding_from_sentence_transformer(model, f'fasttext-{pname}', 'cpu')\n",
    "#\n",
    "# del fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO TFIDF, MFW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
